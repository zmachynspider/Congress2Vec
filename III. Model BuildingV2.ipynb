{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daryazmachynskaya/anaconda/envs/nlp/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/daryazmachynskaya/anaconda/envs/nlp/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from C2V.w2v import MeanEmbeddingVectorizer, TfidfEmbeddingVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import seaborn as sns \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/gov_doc_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congress</th>\n",
       "      <th>date</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>speaker</th>\n",
       "      <th>state</th>\n",
       "      <th>text</th>\n",
       "      <th>protocol</th>\n",
       "      <th>length</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>text_stemmatize</th>\n",
       "      <th>text_lemmatize</th>\n",
       "      <th>t_party</th>\n",
       "      <th>text_stemmatized_haplatized</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112th</td>\n",
       "      <td>20110208</td>\n",
       "      <td>F</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>Ms EDDIE</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>HR551.        Congress has the power t...</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>[hr551, congress, power, enact, legislation, p...</td>\n",
       "      <td>[hr551, congress, power, enact, legisl, pursua...</td>\n",
       "      <td>[hr551, congress, power, enact, legislation, p...</td>\n",
       "      <td>False</td>\n",
       "      <td>[congress, power, enact, legisl, pursuant, fol...</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112th</td>\n",
       "      <td>20120710</td>\n",
       "      <td>F</td>\n",
       "      <td>(D)</td>\n",
       "      <td>Ms HANABUSA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>. Madam Speaker, on June QNTY  the United Stat...</td>\n",
       "      <td>False</td>\n",
       "      <td>165</td>\n",
       "      <td>[madam, speaker, june, qnty, united, states, s...</td>\n",
       "      <td>[madam, speaker, june, qnti, unit, state, supr...</td>\n",
       "      <td>[madam, speaker, june, qnty, united, states, s...</td>\n",
       "      <td>False</td>\n",
       "      <td>[madam, speaker, june, qnti, unit, state, supr...</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.570455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  congress      date gender        party      speaker        state  \\\n",
       "0    112th  20110208      F  unavailable     Ms EDDIE  unavailable   \n",
       "1    112th  20120710      F          (D)  Ms HANABUSA       Hawaii   \n",
       "\n",
       "                                                text protocol  length  \\\n",
       "0          HR551.        Congress has the power t...     True      38   \n",
       "1  . Madam Speaker, on June QNTY  the United Stat...    False     165   \n",
       "\n",
       "                                       text_tokenize  \\\n",
       "0  [hr551, congress, power, enact, legislation, p...   \n",
       "1  [madam, speaker, june, qnty, united, states, s...   \n",
       "\n",
       "                                     text_stemmatize  \\\n",
       "0  [hr551, congress, power, enact, legisl, pursua...   \n",
       "1  [madam, speaker, june, qnti, unit, state, supr...   \n",
       "\n",
       "                                      text_lemmatize t_party  \\\n",
       "0  [hr551, congress, power, enact, legislation, p...   False   \n",
       "1  [madam, speaker, june, qnty, united, states, s...   False   \n",
       "\n",
       "                         text_stemmatized_haplatized  polarity  subjectivity  \n",
       "0  [congress, power, enact, legisl, pursuant, fol... -0.041667      0.075000  \n",
       "1  [madam, speaker, june, qnti, unit, state, supr...  0.084091      0.570455  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*After several iterations, I found that the vast majority of protocol text was not indicative to the speaker but rather indicative of Congression day-by-day business. For this reason, we're going to stick to the more natural speech created by Congress. In addition, after comparting between tokenized, stemmed, and lemmatized text, I found that differences in performance were trivial. Lemmetization and Stemmatization provided virtually identical results depending on the machine learning algorithm selected. Because the computation for the stemmatized dataset was significantly computationally less expensive, I use stick to stemmed text. The biggest effect from preprocessing actually came from removing stopwords. For this reason, stop words are removed from the tokenized, stemmatized, and lemmatized word lists*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(R)    0.533083\n",
       "(D)    0.466917\n",
       "Name: party, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will exclude members without a political par as we know these are data issues\n",
    "X = df[(df['party'] != 'unavailable') & (df['protocol'] == False)]['text_stemmatize']\n",
    "y = df[(df['party'] != 'unavailable') & (df['protocol'] == False)]['party']\n",
    "X, X_test, y, y_test = train_test_split(X,y,test_size = 0.1, random_state=1) \n",
    "y.value_counts()/sum(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The baseline for our party prediction model is 53% if we classify every row as Republican. We don't see any evidence for major class imbalances*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# According to http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/,\n",
    "#it's ok to train w2v on the full dataset\n",
    "model = Word2Vec(df['text_stemmatize'], size=256, min_count=2)\n",
    "#get the word vectors\n",
    "w2v = {w: vec for w, vec in zip(model.index2word, model.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NOTE: found methodolgy on http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "#set up Pipelines\n",
    "\n",
    "b_nb = Pipeline([(\"count vectorizer\", CountVectorizer(analyzer=lambda x: x)), #we pass in the lambda so that we can feed in a manual list of words \n",
    "                    (\"bernoulli nb\", BernoulliNB())])\n",
    "b_nb_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                          (\"bernoulli nb\", BernoulliNB())])\n",
    "\n",
    "m_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)),\n",
    "                    (\"multinomial nb\", MultinomialNB())])\n",
    "m_nb_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                          (\"multinomial nb\", MultinomialNB())])\n",
    "\n",
    "logr = Pipeline([(\"count vectorizer\", CountVectorizer(analyzer=lambda x: x)),\n",
    "                 (\"logistic regression\", LogisticRegression())])\n",
    "logr_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                       (\"logistic regression\", LogisticRegression())])\n",
    "\n",
    "#I tried several kernals but linear gives by far the best results\n",
    "svc = Pipeline([(\"count vectorizer\", CountVectorizer(analyzer=lambda x: x)),\n",
    "                (\"linear svc\", SVC(kernel=\"linear\"))]) \n",
    "svc_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                      (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "\n",
    "#for this step we aren't doing any parameter tunning,so we start by going with a fairly large amount of tress\n",
    "#and hope that this will give us a reasonable baseline for which we can compare this algorithm against the others\n",
    "rf = Pipeline([(\"count vectorizer\", CountVectorizer(analyzer=lambda x: x)),\n",
    "                (\"random forest\", RandomForestClassifier(n_estimators=300))])\n",
    "rf_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                      (\"random forest\", RandomForestClassifier(n_estimators=300))])\n",
    "\n",
    "logr_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"logistic regression\", LogisticRegression())])\n",
    "logr_w2v_tfidf = Pipeline([(\"word2vec tfidf vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"linear svc\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model             score\n",
      "--------------  -------\n",
      "svc_tfidf        0.8315\n",
      "logr_tfidf       0.8281\n",
      "rf               0.8124\n",
      "logr             0.8075\n",
      "rf_tfidf         0.8025\n",
      "m_nb_tfidf       0.8022\n",
      "svc              0.7819\n",
      "m_nb             0.7778\n",
      "logr_w2v_tfidf   0.7546\n",
      "logr_w2v         0.7483\n",
      "b_nb             0.7410\n",
      "b_nb_tfidf       0.7410\n"
     ]
    }
   ],
   "source": [
    "#once more, code stub adapted from tutorial at \n",
    "#http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "models = [(\"b_nb\", b_nb),\n",
    "            (\"b_nb_tfidf\", b_nb_tfidf),\n",
    "            (\"m_nb\", m_nb),\n",
    "            (\"m_nb_tfidf\", m_nb_tfidf),\n",
    "            (\"logr\", logr),\n",
    "            (\"logr_tfidf\", logr_tfidf),\n",
    "            (\"svc\", svc),\n",
    "            (\"svc_tfidf\", svc_tfidf),\n",
    "            (\"rf\", rf),\n",
    "            (\"rf_tfidf\", rf_tfidf),\n",
    "            (\"logr_w2v\", logr_w2v),\n",
    "            (\"logr_w2v_tfidf\", logr_w2v_tfidf)]\n",
    "\n",
    "scores = sorted([(name, cross_val_score(model, X, y, cv=5).mean()) \n",
    "                 for name, model in models], \n",
    "                key=lambda x: -x[1])\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can benchmark the learning rate for our models as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new dataframe\n",
    "model_df = pd.DataFrame(columns=['model','accuracy','sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initiate the sample sizes for the learning rate\n",
    "samples = [10, 50, 150, 650, 3000, 4600]\n",
    "for name, model in models:\n",
    "    train_sizes, _ , valid_scores = learning_curve(model, X, y, train_sizes=samples, cv=5)\n",
    "    for i in range(len(samples)):\n",
    "        dictionary = defaultdict()\n",
    "        dictionary['model'] = name\n",
    "        dictionary['accuracy'] = np.mean(valid_scores[i])\n",
    "        dictionary['sample'] = train_sizes[i]\n",
    "        model_df = model_df.append(dictionary,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = [\"b_nb\",\"b_nb_tfidf\",\"m_nb\",\"m_nb_tfidf\",\"logr\",\"logr_tfidf\",\"svc\",\"svc_tfidf\",\"rf\",\"rf_tfidf\",\"logr_w2v\",\"logr_w2v_tfidf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot(x='sample', y='accuracy', hue='model', \n",
    "                    data=model_df[model_df.model.map(lambda x: x in model_names)])\n",
    "\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"sample size\")\n",
    "fig.set(title=\"Cross-Validation Learning Curves\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Support Vector Machine with a linear kernal outperforms Logistic Regression just slightly, which is perhaps unsurprising considering that they are closely related in derivation. I would argue however that in this case Logistic Regression makes for a better model to use in production. We see that Logistic Regression is more immune to the effects of vectorization. For instance, we see that both the count vectorized model and the tfidf vectorized model perform fairly high for Logistic Regression- the same cannot be said for SVM. Lastly, Logistic Regression is typically easier to communicate. However, purely for the sake of this exercise, we will choose to go with the SVM model due to higher performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haplaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select a different column for X this time\n",
    "X = df[(df['party'] != 'unavailable') & (df['protocol'] == False)]['text_stemmatized_haplatized']\n",
    "y = df[(df['party'] != 'unavailable') & (df['protocol'] == False)]['party']\n",
    "X, X_test, y, y_test = train_test_split(X,y,test_size = 0.1, random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM with a linear kernal performed significantly better than any other alternative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#due to the fact that SVC with a linear kernal performed better, we'll try LinearSVC for further optimization\n",
    "svl_tfidf = Pipeline([(\"tfidf vectorizer\", TfidfVectorizer(analyzer=lambda x: x)),\n",
    "                      (\"fast linear svc\", LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only run the top models\n",
    "models = [(\"svl_tfidf\", svl_tfidf),\n",
    "            (\"logr_tfidf\", logr_tfidf),\n",
    "            (\"svc_tfidf\", svc_tfidf)]\n",
    "\n",
    "scores = sorted([(name, cross_val_score(model, X, y, cv=5).mean()) \n",
    "                 for name, model in models], \n",
    "                key=lambda x: -x[1])\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Removing haplaxes gives us a very slight lift in cross-validated accuracy. In addtion, LinearSVC gives better results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(analyzer=lambda x: x)  \n",
    "X_vec = tv.fit_transform(X)\n",
    "X_vec_test= tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_grid = {'penalty': ['l2'],\n",
    "            'C': [.1,.5,1,10], #default is 1\n",
    "            'loss':['squared_hinge'],\n",
    "            'dual':[True,False],\n",
    "            'random_state': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_gridsearch = GridSearchCV(LinearSVC(),\n",
    "                             svm_grid,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=True,\n",
    "                             scoring='accuracy',\n",
    "                             cv=5)\n",
    "svm_gridsearch.fit(X_vec, y)\n",
    "\n",
    "print(\"best parameters:\", svm_gridsearch.best_params_)\n",
    "\n",
    "best_model = svm_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = np.mean(cross_val_score(best_model, X_vec, y, scoring='accuracy', cv = 5))\n",
    "print(\"Cross-validated accurcy for tuned LinearSVC is {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We get an extremely low lift in accuracy from simply setting 'dual' to True*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_vec,y)\n",
    "y_hat = best_model.predict(X_vec_test)\n",
    "print(\"Test set accuracy for tuned model is:{}\".format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *There is some evidence to suggest that the model has over-fit to the training data (as SVM has a tendency to do). More evidence that Logistic Regression might have been a more robust choice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *265 true negatives (true democrats), 274 true positives (true republicans), 57 false positives (false republicans), and 56 false negatives (false democrats)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"percent of dems correctly classified {} (Recall)\".format(265/(57+265)))\n",
    "print(\"percent of repubs correctly classified {} (Recall)\".format(274/(56+274)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Our model has a slightly easier time (for Recall) classifying Republicans , but it is within the margin of error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *One our algorithm makes a classification, the precision/quality of that classification by class is essentially identical (83%), but in terms of recall, there's slightly better performance for Republicans as discussed above, but the difference is extremely close.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve([1 if x == '(R)' else 0 for x in y_test],\n",
    "          [1 if x == '(R)' else 0 for x in y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\n",
    "plt.title(\"ROC plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#according to http://stats.stackexchange.com/questions/39243/how-does-one-interpret-svm-feature-weights\n",
    "#we can use this to estimate feature importance\n",
    "best_model.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stems that increased likelihood of getting classified as Democrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zip(tv.get_feature_names(),best_model.coef_[0])\n",
    "dem_words = sorted(zip(tv.get_feature_names(),best_model.coef_[0]), \n",
    "        key=lambda x: x[1])[0:30]\n",
    "print(tabulate(dem_words, floatfmt=\".4f\", headers=(\"Stem\", 'Coefficient')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stems that increased likelihood of getting classified as Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep_words = sorted(zip(tv.get_feature_names(),best_model.coef_[0]), \n",
    "        key=lambda x: -x[1])[0:30]\n",
    "print(tabulate(rep_words, floatfmt=\".4f\", headers=(\"Stem\", 'Coefficient')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *It is perhaps once more unremarkable that our model most heavily relies on state references to learn whether the representative is republican or democrat*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For extra reference, HR138 was a bill that gave tax credits for natural gas and HR1161 reaffirmed that states had the power to regulate alcohol*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "---\n",
    "\n",
    "> Inititally I thought more features would improve the model; however, I found that there was essentially no lift from adding these features. This is why I moved this section to the Appendix. It is, however, useful to include this for reference on how to use Pipelines to combine vectorized and non-vectorized features into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can add sentiment from TextBlob, which returns both polarity as well as subjectivity. Also, we will add the word count, captured by length. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subjectivity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['polarity'] = df.apply(lambda row: get_polarity(row['text']), axis=1)\n",
    "df['subjectivity'] = df.apply(lambda row: get_subjectivity(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select more columsn for X this time\n",
    "X = df[(df['party'] != 'unavailable') & (df['protocol'] == False)][['text_stemmatized_haplatized','length','polarity','subjectivity']]\n",
    "y = df[(df['party'] != 'unavailable') & (df['protocol'] == False)]['party']\n",
    "X, X_test, y, y_test = train_test_split(X,y,test_size = 0.1, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this class will allows us to use FeatureUnion in the Pipeline to create a matrix that combines both the vectorized\n",
    "#TFIDF matrix with other features such as polarity:\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Given dataframe, selects columsn by key for specific selections\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_stemmatized_haplatized')),\n",
    "                ('tfidf', TfidfVectorizer(analyzer=lambda x: x)) #we only vectorize the text\n",
    "            ])),\n",
    "\n",
    "            ('length', Pipeline([ #we don't need to vectorize these\n",
    "                ('selector', ItemSelector(key=['length','polarity','subjectivity']))\n",
    "            ]))\n",
    "\n",
    "        ],\n",
    "\n",
    "    )),\n",
    "\n",
    "    ('lgr', LogisticRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(pipeline, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *This cross-validation error compares to the .8283 we had for Logisitic Regression TFIDF without these added features. It is unlikely that our two new features add value to our model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)\n",
    "yhat = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, yhat))\n",
    "print(accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Although this model does better in returning Republicans (85% Recall), it does so at the expense of Democrat Recall. In addition, it's Republican precision is lower than our best model. *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
